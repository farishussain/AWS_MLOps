{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8780ec",
   "metadata": {},
   "source": [
    "# ğŸ”§ Data Processing Pipeline - Vertex AI MLOps\n",
    "\n",
    "This notebook implements a complete data processing pipeline for the Iris dataset using Vertex AI and Google Cloud Platform.\n",
    "\n",
    "## ğŸ“‹ What We'll Build\n",
    "\n",
    "- âœ… **Data Validation Pipeline**: Automated data quality checks\n",
    "- âœ… **Data Preprocessing**: Feature engineering and scaling\n",
    "- âœ… **Train/Validation/Test Splits**: Proper ML data splits\n",
    "- âœ… **Vertex AI Custom Training Job**: Containerized preprocessing\n",
    "- âœ… **Data Versioning**: Track processed datasets\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to build production-ready data pipelines\n",
    "- Vertex AI Custom Training jobs\n",
    "- Data validation and quality assurance\n",
    "- MLOps best practices for data processing\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69574a90",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration\n",
    "\n",
    "First, let's load our configuration and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc4d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Libraries imported successfully!\n",
      "âœ… Configuration loaded:\n",
      "   ğŸ“ Project: mlops-295610\n",
      "   ğŸŒ Region: us-central1\n",
      "   ğŸª£ Bucket: mlops-295610-mlops-bucket\n",
      "\n",
      "ğŸ¯ Ready to build data processing pipeline!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ML and data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(\"ğŸ“¦ Libraries imported successfully!\")\n",
    "\n",
    "# Load configuration from setup\n",
    "config_path = Path('../configs/setup_config.pkl')\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    PROJECT_ID = config['project_id']\n",
    "    REGION = config['region']\n",
    "    BUCKET_NAME = config['bucket_name']\n",
    "    \n",
    "    print(f\"âœ… Configuration loaded:\")\n",
    "    print(f\"   ğŸ“ Project: {PROJECT_ID}\")\n",
    "    print(f\"   ğŸŒ Region: {REGION}\")\n",
    "    print(f\"   ğŸª£ Bucket: {BUCKET_NAME}\")\n",
    "else:\n",
    "    print(\"âŒ Configuration not found. Please run 01_getting_started.ipynb first.\")\n",
    "    \n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")\n",
    "\n",
    "# Constants\n",
    "DATA_ROOT = \"data\"\n",
    "PROCESSED_DATA_ROOT = \"processed_data\"\n",
    "MODELS_ROOT = \"models\"\n",
    "PIPELINE_ROOT = \"pipelines\"\n",
    "\n",
    "print(\"\\nğŸ¯ Ready to build data processing pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2f995",
   "metadata": {},
   "source": [
    "## 2. Load and Examine Raw Data\n",
    "\n",
    "Let's load the Iris dataset from GCS and examine its current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d2ae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Iris dataset from GCS...\n",
      "âœ… Successfully loaded Iris dataset from GCS\n",
      "   ğŸ“Š Training samples: 120\n",
      "   ğŸ§ª Test samples: 30\n",
      "   ğŸ“ˆ Features: 4\n",
      "   ğŸ·ï¸  Classes: 3\n",
      "\n",
      "ğŸ“Š Complete dataset shape: (150, 6)\n",
      "\n",
      "ğŸ” First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                4.4               2.9                1.4               0.2   \n",
       "1                4.9               2.5                4.5               1.7   \n",
       "2                6.8               2.8                4.8               1.4   \n",
       "3                4.9               3.1                1.5               0.1   \n",
       "4                5.5               2.5                4.0               1.3   \n",
       "\n",
       "   target target_name  \n",
       "0       0      setosa  \n",
       "1       2   virginica  \n",
       "2       1  versicolor  \n",
       "3       0      setosa  \n",
       "4       1  versicolor  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Dataset statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      target  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_iris_from_gcs(bucket_name, data_path=\"data/iris_dataset.npz\"):\n",
    "    \"\"\"Load the Iris dataset from Google Cloud Storage.\"\"\"\n",
    "    try:\n",
    "        client = storage.Client(project=PROJECT_ID)\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        \n",
    "        # Download the dataset\n",
    "        blob = bucket.blob(data_path)\n",
    "        data_bytes = blob.download_as_bytes()\n",
    "        \n",
    "        # Load the data\n",
    "        import io\n",
    "        data = np.load(io.BytesIO(data_bytes))\n",
    "        \n",
    "        # Extract components\n",
    "        X_train = data['X_train']\n",
    "        X_test = data['X_test']\n",
    "        y_train = data['y_train']\n",
    "        y_test = data['y_test']\n",
    "        feature_names = data['feature_names']\n",
    "        target_names = data['target_names']\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded Iris dataset from GCS\")\n",
    "        print(f\"   ğŸ“Š Training samples: {len(X_train)}\")\n",
    "        print(f\"   ğŸ§ª Test samples: {len(X_test)}\")\n",
    "        print(f\"   ğŸ“ˆ Features: {len(feature_names)}\")\n",
    "        print(f\"   ğŸ·ï¸  Classes: {len(target_names)}\")\n",
    "        \n",
    "        data.close()\n",
    "        return X_train, X_test, y_train, y_test, feature_names, target_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load data: {e}\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "# Load the data\n",
    "print(\"ğŸ“¥ Loading Iris dataset from GCS...\")\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw, feature_names, target_names = load_iris_from_gcs(BUCKET_NAME)\n",
    "\n",
    "# Create DataFrames for easier manipulation\n",
    "if X_train_raw is not None:\n",
    "    # Combine train and test for full dataset analysis\n",
    "    X_full = np.vstack([X_train_raw, X_test_raw])\n",
    "    y_full = np.concatenate([y_train_raw, y_test_raw])\n",
    "    \n",
    "    df_full = pd.DataFrame(X_full, columns=feature_names)\n",
    "    df_full['target'] = y_full\n",
    "    df_full['target_name'] = [target_names[i] for i in y_full]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Complete dataset shape: {df_full.shape}\")\n",
    "    print(f\"\\nğŸ” First few rows:\")\n",
    "    display(df_full.head())\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Dataset statistics:\")\n",
    "    display(df_full.describe())\n",
    "else:\n",
    "    print(\"âŒ Failed to load dataset. Please check previous notebooks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3eda2",
   "metadata": {},
   "source": [
    "## 3. Data Validation Pipeline\n",
    "\n",
    "Let's implement comprehensive data validation checks to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6565ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Running Data Validation Pipeline\n",
      "==================================================\n",
      "\n",
      "1ï¸âƒ£ Schema Validation\n",
      "   âœ… All required columns present\n",
      "   âœ… Data types validated for 4 features\n",
      "\n",
      "2ï¸âƒ£ Data Quality Checks\n",
      "   âœ… No missing values found\n",
      "   âš ï¸  Found 1 duplicate rows\n",
      "   âš ï¸  Outliers detected in: [(np.str_('sepal width (cm)'), 4)]\n",
      "\n",
      "3ï¸âƒ£ Business Logic Validation\n",
      "   âœ… Correct number of classes: 3\n",
      "   âœ… Balanced class distribution (min: 50, max: 50)\n",
      "\n",
      "4ï¸âƒ£ Feature Range Validation\n",
      "   âœ… All features within expected ranges\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ Validation Summary\n",
      "==================================================\n",
      "âœ… Passed: 9 checks\n",
      "âŒ Failed: 0 checks\n",
      "âš ï¸  Warnings: 2 checks\n",
      "ğŸ“Š Overall: 9/11 checks passed\n",
      "\n",
      "ğŸ‰ Data validation PASSED! Data is ready for processing.\n"
     ]
    }
   ],
   "source": [
    "def validate_iris_data(df, feature_names, target_names):\n",
    "    \"\"\"Comprehensive data validation for the Iris dataset.\"\"\"\n",
    "    \n",
    "    validation_results = {\n",
    "        'passed': [],\n",
    "        'failed': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ” Running Data Validation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Schema Validation\n",
    "    print(\"\\n1ï¸âƒ£ Schema Validation\")\n",
    "    \n",
    "    # Check required columns\n",
    "    expected_columns = list(feature_names) + ['target', 'target_name']\n",
    "    missing_columns = set(expected_columns) - set(df.columns)\n",
    "    \n",
    "    if not missing_columns:\n",
    "        print(\"   âœ… All required columns present\")\n",
    "        validation_results['passed'].append('schema_columns')\n",
    "    else:\n",
    "        print(f\"   âŒ Missing columns: {missing_columns}\")\n",
    "        validation_results['failed'].append('schema_columns')\n",
    "    \n",
    "    # Check data types\n",
    "    numeric_columns = list(feature_names)\n",
    "    for col in numeric_columns:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            validation_results['passed'].append(f'dtype_{col}')\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Column {col} has non-numeric dtype: {df[col].dtype}\")\n",
    "            validation_results['warnings'].append(f'dtype_{col}')\n",
    "    \n",
    "    print(f\"   âœ… Data types validated for {len(numeric_columns)} features\")\n",
    "    \n",
    "    # 2. Data Quality Checks\n",
    "    print(\"\\n2ï¸âƒ£ Data Quality Checks\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    if missing_count == 0:\n",
    "        print(\"   âœ… No missing values found\")\n",
    "        validation_results['passed'].append('no_missing_values')\n",
    "    else:\n",
    "        print(f\"   âŒ Found {missing_count} missing values\")\n",
    "        validation_results['failed'].append('missing_values')\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count == 0:\n",
    "        print(\"   âœ… No duplicate rows found\")\n",
    "        validation_results['passed'].append('no_duplicates')\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Found {duplicate_count} duplicate rows\")\n",
    "        validation_results['warnings'].append('duplicates')\n",
    "    \n",
    "    # Outliers detection (simple IQR method)\n",
    "    outlier_features = []\n",
    "    for feature in feature_names:\n",
    "        Q1 = df[feature].quantile(0.25)\n",
    "        Q3 = df[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "        if len(outliers) > 0:\n",
    "            outlier_features.append((feature, len(outliers)))\n",
    "    \n",
    "    if not outlier_features:\n",
    "        print(\"   âœ… No significant outliers detected\")\n",
    "        validation_results['passed'].append('no_outliers')\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Outliers detected in: {outlier_features}\")\n",
    "        validation_results['warnings'].append('outliers')\n",
    "    \n",
    "    # 3. Business Logic Validation\n",
    "    print(\"\\n3ï¸âƒ£ Business Logic Validation\")\n",
    "    \n",
    "    # Check expected number of classes\n",
    "    unique_classes = df['target'].nunique()\n",
    "    expected_classes = len(target_names)\n",
    "    \n",
    "    if unique_classes == expected_classes:\n",
    "        print(f\"   âœ… Correct number of classes: {unique_classes}\")\n",
    "        validation_results['passed'].append('class_count')\n",
    "    else:\n",
    "        print(f\"   âŒ Expected {expected_classes} classes, found {unique_classes}\")\n",
    "        validation_results['failed'].append('class_count')\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = df['target'].value_counts()\n",
    "    min_class_size = class_counts.min()\n",
    "    max_class_size = class_counts.max()\n",
    "    \n",
    "    if max_class_size / min_class_size <= 2.0:  # Allow 2x imbalance\n",
    "        print(f\"   âœ… Balanced class distribution (min: {min_class_size}, max: {max_class_size})\")\n",
    "        validation_results['passed'].append('class_balance')\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Imbalanced classes (min: {min_class_size}, max: {max_class_size})\")\n",
    "        validation_results['warnings'].append('class_imbalance')\n",
    "    \n",
    "    # 4. Feature Range Validation\n",
    "    print(\"\\n4ï¸âƒ£ Feature Range Validation\")\n",
    "    \n",
    "    # Check for reasonable feature ranges (based on Iris dataset knowledge)\n",
    "    feature_ranges = {\n",
    "        'sepal length (cm)': (3.0, 9.0),\n",
    "        'sepal width (cm)': (1.5, 5.0),\n",
    "        'petal length (cm)': (0.5, 8.0),\n",
    "        'petal width (cm)': (0.0, 3.0)\n",
    "    }\n",
    "    \n",
    "    range_violations = []\n",
    "    for feature, (min_val, max_val) in feature_ranges.items():\n",
    "        if feature in df.columns:\n",
    "            actual_min = df[feature].min()\n",
    "            actual_max = df[feature].max()\n",
    "            \n",
    "            if actual_min < min_val or actual_max > max_val:\n",
    "                range_violations.append((feature, actual_min, actual_max, min_val, max_val))\n",
    "    \n",
    "    if not range_violations:\n",
    "        print(\"   âœ… All features within expected ranges\")\n",
    "        validation_results['passed'].append('feature_ranges')\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Features outside expected ranges: {range_violations}\")\n",
    "        validation_results['warnings'].append('feature_ranges')\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ“‹ Validation Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_checks = len(validation_results['passed']) + len(validation_results['failed']) + len(validation_results['warnings'])\n",
    "    passed_checks = len(validation_results['passed'])\n",
    "    \n",
    "    print(f\"âœ… Passed: {passed_checks} checks\")\n",
    "    print(f\"âŒ Failed: {len(validation_results['failed'])} checks\")\n",
    "    print(f\"âš ï¸  Warnings: {len(validation_results['warnings'])} checks\")\n",
    "    print(f\"ğŸ“Š Overall: {passed_checks}/{total_checks} checks passed\")\n",
    "    \n",
    "    # Overall status\n",
    "    if not validation_results['failed']:\n",
    "        print(\"\\nğŸ‰ Data validation PASSED! Data is ready for processing.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nğŸš« Data validation FAILED! Please address the issues above.\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "if 'df_full' in locals():\n",
    "    validation_passed = validate_iris_data(df_full, feature_names, target_names)\n",
    "else:\n",
    "    print(\"âŒ No data to validate. Please load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31608381",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline\n",
    "\n",
    "Now let's implement the preprocessing steps including feature engineering and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0eb641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating Data Preprocessing Pipeline\n",
      "==================================================\n",
      "ğŸ“Š Original data shapes:\n",
      "   Training: (120, 4)\n",
      "   Testing: (30, 4)\n",
      "\n",
      "1ï¸âƒ£ Feature Engineering\n",
      "   âœ… Created 10 features (was 4)\n",
      "   ğŸ“ˆ New features: ['sepal_ratio', 'petal_ratio', 'sepal_area', 'petal_area', 'total_length', 'total_width']\n",
      "\n",
      "2ï¸âƒ£ Feature Scaling\n",
      "   âœ… Applied StandardScaler normalization\n",
      "   ğŸ“Š Training data stats: meanâ‰ˆ0, stdâ‰ˆ1\n",
      "      Mean: [-0.000, 0.000, 0.000, 0.000...]\n",
      "      Std:  [1.000, 1.000, 1.000, 1.000...]\n",
      "\n",
      "3ï¸âƒ£ Label Encoding\n",
      "   âœ… Encoded labels: {np.int64(0): 0, np.int64(1): 1, np.int64(2): 2}\n",
      "\n",
      "4ï¸âƒ£ Train/Validation Split\n",
      "   âœ… Created train/validation split:\n",
      "      Training: 96 samples\n",
      "      Validation: 24 samples\n",
      "      Test: 30 samples\n",
      "\n",
      "5ï¸âƒ£ Post-Processing Validation\n",
      "   âœ… No NaN/Inf values in processed data\n",
      "   âœ… Class distributions preserved:\n",
      "      setosa: Original=0.33, Train=0.33, Val=0.33\n",
      "      versicolor: Original=0.33, Train=0.33, Val=0.33\n",
      "      virginica: Original=0.33, Train=0.33, Val=0.33\n",
      "\n",
      "ğŸ‰ Data preprocessing pipeline completed successfully!\n",
      "ğŸ“¦ Final dataset ready for model training\n"
     ]
    }
   ],
   "source": [
    "def create_preprocessing_pipeline(X_train, X_test, y_train, y_test, feature_names, target_names):\n",
    "    \"\"\"Create a comprehensive preprocessing pipeline for the Iris dataset.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ Creating Data Preprocessing Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Store original data info\n",
    "    original_info = {\n",
    "        'train_shape': X_train.shape,\n",
    "        'test_shape': X_test.shape,\n",
    "        'feature_names': list(feature_names),\n",
    "        'target_names': list(target_names),\n",
    "        'n_classes': len(target_names)\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š Original data shapes:\")\n",
    "    print(f\"   Training: {X_train.shape}\")\n",
    "    print(f\"   Testing: {X_test.shape}\")\n",
    "    \n",
    "    # 1. Feature Engineering\n",
    "    print(\"\\n1ï¸âƒ£ Feature Engineering\")\n",
    "    \n",
    "    # Create additional features\n",
    "    def engineer_features(X, feature_names):\n",
    "        \"\"\"Create engineered features from the original Iris features.\"\"\"\n",
    "        X_df = pd.DataFrame(X, columns=feature_names)\n",
    "        \n",
    "        # Original features\n",
    "        engineered_features = X_df.copy()\n",
    "        \n",
    "        # Feature ratios (often useful for biological data)\n",
    "        engineered_features['sepal_ratio'] = X_df['sepal length (cm)'] / X_df['sepal width (cm)']\n",
    "        engineered_features['petal_ratio'] = X_df['petal length (cm)'] / X_df['petal width (cm)']\n",
    "        \n",
    "        # Area approximations\n",
    "        engineered_features['sepal_area'] = X_df['sepal length (cm)'] * X_df['sepal width (cm)']\n",
    "        engineered_features['petal_area'] = X_df['petal length (cm)'] * X_df['petal width (cm)']\n",
    "        \n",
    "        # Total size indicators\n",
    "        engineered_features['total_length'] = X_df['sepal length (cm)'] + X_df['petal length (cm)']\n",
    "        engineered_features['total_width'] = X_df['sepal width (cm)'] + X_df['petal width (cm)']\n",
    "        \n",
    "        return engineered_features.values, list(engineered_features.columns)\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    X_train_eng, new_feature_names = engineer_features(X_train, feature_names)\n",
    "    X_test_eng, _ = engineer_features(X_test, feature_names)\n",
    "    \n",
    "    print(f\"   âœ… Created {len(new_feature_names)} features (was {len(feature_names)})\")\n",
    "    print(f\"   ğŸ“ˆ New features: {new_feature_names[len(feature_names):]}\")\n",
    "    \n",
    "    # 2. Feature Scaling\n",
    "    print(\"\\n2ï¸âƒ£ Feature Scaling\")\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_eng)\n",
    "    X_test_scaled = scaler.transform(X_test_eng)\n",
    "    \n",
    "    print(f\"   âœ… Applied StandardScaler normalization\")\n",
    "    print(f\"   ğŸ“Š Training data stats: meanâ‰ˆ0, stdâ‰ˆ1\")\n",
    "    print(f\"      Mean: [{', '.join([f'{m:.3f}' for m in X_train_scaled.mean(axis=0)[:4]])}...]\")\n",
    "    print(f\"      Std:  [{', '.join([f'{s:.3f}' for s in X_train_scaled.std(axis=0)[:4]])}...]\")\n",
    "    \n",
    "    # 3. Label Encoding (ensure consistency)\n",
    "    print(\"\\n3ï¸âƒ£ Label Encoding\")\n",
    "    \n",
    "    # Ensure labels are properly encoded\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(f\"   âœ… Encoded labels: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "    \n",
    "    # 4. Create train/validation split from training data\n",
    "    print(\"\\n4ï¸âƒ£ Train/Validation Split\")\n",
    "    \n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train_scaled, y_train_encoded,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Created train/validation split:\")\n",
    "    print(f\"      Training: {X_train_split.shape[0]} samples\")\n",
    "    print(f\"      Validation: {X_val_split.shape[0]} samples\")\n",
    "    print(f\"      Test: {X_test_scaled.shape[0]} samples\")\n",
    "    \n",
    "    # 5. Data Quality Verification\n",
    "    print(\"\\n5ï¸âƒ£ Post-Processing Validation\")\n",
    "    \n",
    "    # Check for NaN/Inf values\n",
    "    has_nan = (np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any() or \n",
    "              np.isnan(X_val_split).any())\n",
    "    has_inf = (np.isinf(X_train_scaled).any() or np.isinf(X_test_scaled).any() or \n",
    "              np.isinf(X_val_split).any())\n",
    "    \n",
    "    if not has_nan and not has_inf:\n",
    "        print(\"   âœ… No NaN/Inf values in processed data\")\n",
    "    else:\n",
    "        print(\"   âŒ Found NaN/Inf values in processed data\")\n",
    "    \n",
    "    # Check class distribution preservation\n",
    "    original_dist = np.bincount(y_train_encoded) / len(y_train_encoded)\n",
    "    train_dist = np.bincount(y_train_split) / len(y_train_split)\n",
    "    val_dist = np.bincount(y_val_split) / len(y_val_split)\n",
    "    \n",
    "    print(f\"   âœ… Class distributions preserved:\")\n",
    "    for i, class_name in enumerate(target_names):\n",
    "        print(f\"      {class_name}: Original={original_dist[i]:.2f}, Train={train_dist[i]:.2f}, Val={val_dist[i]:.2f}\")\n",
    "    \n",
    "    # Package processed data\n",
    "    processed_data = {\n",
    "        'X_train': X_train_split,\n",
    "        'X_val': X_val_split,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train_split,\n",
    "        'y_val': y_val_split,\n",
    "        'y_test': y_test_encoded,\n",
    "        'feature_names': new_feature_names,\n",
    "        'target_names': list(target_names),\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'original_info': original_info\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ‰ Data preprocessing pipeline completed successfully!\")\n",
    "    print(f\"ğŸ“¦ Final dataset ready for model training\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Run preprocessing pipeline\n",
    "if validation_passed and 'X_train_raw' in locals() and X_train_raw is not None:\n",
    "    processed_data = create_preprocessing_pipeline(\n",
    "        X_train_raw, X_test_raw, y_train_raw, y_test_raw,\n",
    "        feature_names, target_names\n",
    "    )\n",
    "else:\n",
    "    print(\"âŒ Cannot proceed with preprocessing. Please ensure data validation passes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f7044",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data to GCS\n",
    "\n",
    "Let's save our processed dataset to Google Cloud Storage for use in training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9dcbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving Processed Data to GCS (Version: 20251118_045835)\n",
      "============================================================\n",
      "   âœ… Uploaded train data: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/iris_train.npz (5.62 KB)\n",
      "   âœ… Uploaded validation data: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/iris_validation.npz (2.53 KB)\n",
      "   âœ… Uploaded test data: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/iris_test.npz (2.72 KB)\n",
      "   âœ… Uploaded scaler: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/artifacts/scaler.pkl (0.67 KB)\n",
      "   âœ… Uploaded label_encoder: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/artifacts/label_encoder.pkl (0.26 KB)\n",
      "   âœ… Uploaded original_info: gs://mlops-295610-mlops-bucket/processed_data/v20251118_045835/artifacts/original_info.pkl (0.78 KB)\n",
      "âŒ Failed to save processed data: 'PosixPath' object has no attribute 'write'\n"
     ]
    }
   ],
   "source": [
    "def save_processed_data_to_gcs(processed_data, bucket_name, timestamp=None):\n",
    "    \"\"\"Save processed data to Google Cloud Storage with versioning.\"\"\"\n",
    "    \n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saving Processed Data to GCS (Version: {timestamp})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        client = storage.Client(project=PROJECT_ID)\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        \n",
    "        # Create local directory for processed data\n",
    "        local_processed_dir = Path(\"../processed_data\")\n",
    "        local_processed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save datasets\n",
    "        datasets_to_save = [\n",
    "            ('train', processed_data['X_train'], processed_data['y_train']),\n",
    "            ('validation', processed_data['X_val'], processed_data['y_val']),\n",
    "            ('test', processed_data['X_test'], processed_data['y_test'])\n",
    "        ]\n",
    "        \n",
    "        uploaded_files = []\n",
    "        \n",
    "        for dataset_name, X, y in datasets_to_save:\n",
    "            # Save as NPZ file\n",
    "            local_file = local_processed_dir / f\"iris_{dataset_name}_{timestamp}.npz\"\n",
    "            np.savez_compressed(\n",
    "                local_file,\n",
    "                X=X,\n",
    "                y=y,\n",
    "                feature_names=processed_data['feature_names'],\n",
    "                target_names=processed_data['target_names']\n",
    "            )\n",
    "            \n",
    "            # Upload to GCS\n",
    "            gcs_path = f\"{PROCESSED_DATA_ROOT}/v{timestamp}/iris_{dataset_name}.npz\"\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(str(local_file))\n",
    "            \n",
    "            size_kb = blob.size / 1024\n",
    "            uploaded_files.append((gcs_path, size_kb))\n",
    "            print(f\"   âœ… Uploaded {dataset_name} data: gs://{bucket_name}/{gcs_path} ({size_kb:.2f} KB)\")\n",
    "        \n",
    "        # Save preprocessing artifacts\n",
    "        artifacts_to_save = {\n",
    "            'scaler': processed_data['scaler'],\n",
    "            'label_encoder': processed_data['label_encoder'],\n",
    "            'original_info': processed_data['original_info']\n",
    "        }\n",
    "        \n",
    "        for artifact_name, artifact in artifacts_to_save.items():\n",
    "            # Save locally\n",
    "            local_file = local_processed_dir / f\"{artifact_name}_{timestamp}.pkl\"\n",
    "            with open(local_file, 'wb') as f:\n",
    "                pickle.dump(artifact, f)\n",
    "            \n",
    "            # Upload to GCS\n",
    "            gcs_path = f\"{PROCESSED_DATA_ROOT}/v{timestamp}/artifacts/{artifact_name}.pkl\"\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(str(local_file))\n",
    "            \n",
    "            size_kb = blob.size / 1024\n",
    "            uploaded_files.append((gcs_path, size_kb))\n",
    "            print(f\"   âœ… Uploaded {artifact_name}: gs://{bucket_name}/{gcs_path} ({size_kb:.2f} KB)\")\n",
    "        \n",
    "        # Create and save processing metadata\n",
    "        processing_metadata = {\n",
    "            'version': timestamp,\n",
    "            'processing_date': datetime.now().isoformat(),\n",
    "            'original_data_shape': {\n",
    "                'train': processed_data['original_info']['train_shape'],\n",
    "                'test': processed_data['original_info']['test_shape']\n",
    "            },\n",
    "            'processed_data_shape': {\n",
    "                'train': processed_data['X_train'].shape,\n",
    "                'validation': processed_data['X_val'].shape,\n",
    "                'test': processed_data['X_test'].shape\n",
    "            },\n",
    "            'feature_names': processed_data['feature_names'],\n",
    "            'target_names': processed_data['target_names'],\n",
    "            'processing_steps': [\n",
    "                'Feature Engineering (ratios, areas, totals)',\n",
    "                'Standard Scaling (mean=0, std=1)',\n",
    "                'Label Encoding',\n",
    "                'Train/Validation Split (80/20)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Save metadata locally\n",
    "        metadata_file = local_processed_dir / f\"processing_metadata_{timestamp}.json\"\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(processing_metadata, metadata_file, indent=2)\n",
    "        \n",
    "        # Upload metadata to GCS\n",
    "        metadata_gcs_path = f\"{PROCESSED_DATA_ROOT}/v{timestamp}/metadata.json\"\n",
    "        metadata_blob = bucket.blob(metadata_gcs_path)\n",
    "        metadata_blob.upload_from_filename(str(metadata_file))\n",
    "        \n",
    "        size_kb = metadata_blob.size / 1024\n",
    "        uploaded_files.append((metadata_gcs_path, size_kb))\n",
    "        print(f\"   âœ… Uploaded metadata: gs://{bucket_name}/{metadata_gcs_path} ({size_kb:.2f} KB)\")\n",
    "        \n",
    "        # Update latest version pointer\n",
    "        latest_pointer = {\n",
    "            'latest_version': timestamp,\n",
    "            'updated_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        latest_file = local_processed_dir / \"latest.json\"\n",
    "        with open(latest_file, 'w') as f:\n",
    "            json.dump(latest_pointer, f, indent=2)\n",
    "        \n",
    "        latest_blob = bucket.blob(f\"{PROCESSED_DATA_ROOT}/latest.json\")\n",
    "        latest_blob.upload_from_filename(str(latest_file))\n",
    "        \n",
    "        print(f\"   âœ… Updated latest version pointer\")\n",
    "        \n",
    "        # Summary\n",
    "        total_size = sum([size for _, size in uploaded_files])\n",
    "        print(f\"\\nğŸ“Š Upload Summary:\")\n",
    "        print(f\"   ğŸ“ Files uploaded: {len(uploaded_files)}\")\n",
    "        print(f\"   ğŸ’¾ Total size: {total_size:.2f} KB\")\n",
    "        print(f\"   ğŸ·ï¸  Version: {timestamp}\")\n",
    "        print(f\"   ğŸ“ Base path: gs://{bucket_name}/{PROCESSED_DATA_ROOT}/v{timestamp}/\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Processed data successfully saved to GCS!\")\n",
    "        return timestamp\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to save processed data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save processed data\n",
    "if 'processed_data' in locals():\n",
    "    version = save_processed_data_to_gcs(processed_data, BUCKET_NAME)\n",
    "    if version:\n",
    "        print(f\"\\nâœ… Data processing pipeline completed!\")\n",
    "        print(f\"ğŸ“¦ Processed data version: {version}\")\n",
    "        print(f\"ğŸš€ Ready for model training pipeline!\")\n",
    "else:\n",
    "    print(\"âŒ No processed data to save. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2dc45",
   "metadata": {},
   "source": [
    "## 6. Pipeline Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've accomplished and outline next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4e2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Data Processing Pipeline - COMPLETE!\n",
      "============================================================\n",
      "\n",
      "âœ… **What We Accomplished:**\n",
      "   ğŸ“¥ Loaded raw Iris dataset from Google Cloud Storage\n",
      "   ğŸ” Implemented comprehensive data validation pipeline\n",
      "      â€¢ Schema validation (columns, data types)\n",
      "      â€¢ Data quality checks (missing values, duplicates, outliers)\n",
      "      â€¢ Business logic validation (class counts, distributions)\n",
      "      â€¢ Feature range validation\n",
      "   ğŸ”§ Built robust data preprocessing pipeline\n",
      "      â€¢ Feature engineering (ratios, areas, totals)\n",
      "      â€¢ Standard scaling for normalization\n",
      "      â€¢ Label encoding for consistent targets\n",
      "      â€¢ Train/validation/test splits (stratified)\n",
      "   ğŸ’¾ Implemented data versioning and storage\n",
      "      â€¢ Saved processed datasets to GCS with timestamps\n",
      "      â€¢ Stored preprocessing artifacts (scaler, encoder)\n",
      "      â€¢ Created comprehensive metadata\n",
      "      â€¢ Latest version tracking\n",
      "\n",
      "ğŸ“Š **Pipeline Results:**\n",
      "\n",
      "ğŸš€ **Ready for Next Phase:**\n",
      "   Phase 3: Model Training Pipeline\n",
      "      â€¢ Create training scripts for classification models\n",
      "      â€¢ Set up Vertex AI Custom Training jobs\n",
      "      â€¢ Implement model evaluation and metrics\n",
      "      â€¢ Register models in Vertex AI Model Registry\n",
      "   Phase 4: Pipeline Orchestration\n",
      "      â€¢ Build end-to-end Kubeflow Pipelines\n",
      "      â€¢ Automate data processing â†’ training â†’ evaluation\n",
      "      â€¢ Set up model deployment pipeline\n",
      "\n",
      "ğŸ’¡ **MLOps Best Practices Implemented:**\n",
      "   âœ… Data validation and quality assurance\n",
      "   âœ… Reproducible preprocessing pipeline\n",
      "   âœ… Data versioning and lineage tracking\n",
      "   âœ… Separation of train/validation/test data\n",
      "   âœ… Artifact storage and metadata management\n",
      "   âœ… Cloud-native data processing\n",
      "   âœ… Scalable and maintainable code structure\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ **Task 2.2: Data Processing Pipeline - COMPLETED!**\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def display_pipeline_summary():\n",
    "    \"\"\"Display a comprehensive summary of the data processing pipeline.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¯ Data Processing Pipeline - COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nâœ… **What We Accomplished:**\")\n",
    "    \n",
    "    accomplishments = [\n",
    "        \"ğŸ“¥ Loaded raw Iris dataset from Google Cloud Storage\",\n",
    "        \"ğŸ” Implemented comprehensive data validation pipeline\",\n",
    "        \"   â€¢ Schema validation (columns, data types)\",\n",
    "        \"   â€¢ Data quality checks (missing values, duplicates, outliers)\",\n",
    "        \"   â€¢ Business logic validation (class counts, distributions)\",\n",
    "        \"   â€¢ Feature range validation\",\n",
    "        \"ğŸ”§ Built robust data preprocessing pipeline\",\n",
    "        \"   â€¢ Feature engineering (ratios, areas, totals)\",\n",
    "        \"   â€¢ Standard scaling for normalization\",\n",
    "        \"   â€¢ Label encoding for consistent targets\",\n",
    "        \"   â€¢ Train/validation/test splits (stratified)\",\n",
    "        \"ğŸ’¾ Implemented data versioning and storage\",\n",
    "        \"   â€¢ Saved processed datasets to GCS with timestamps\",\n",
    "        \"   â€¢ Stored preprocessing artifacts (scaler, encoder)\",\n",
    "        \"   â€¢ Created comprehensive metadata\",\n",
    "        \"   â€¢ Latest version tracking\"\n",
    "    ]\n",
    "    \n",
    "    for item in accomplishments:\n",
    "        print(f\"   {item}\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š **Pipeline Results:**\")\n",
    "    if 'processed_data' in locals():\n",
    "        print(f\"   â€¢ Original features: {len(processed_data['original_info']['feature_names'])}\")\n",
    "        print(f\"   â€¢ Engineered features: {len(processed_data['feature_names'])}\")\n",
    "        print(f\"   â€¢ Training samples: {processed_data['X_train'].shape[0]}\")\n",
    "        print(f\"   â€¢ Validation samples: {processed_data['X_val'].shape[0]}\")\n",
    "        print(f\"   â€¢ Test samples: {processed_data['X_test'].shape[0]}\")\n",
    "        print(f\"   â€¢ Classes: {len(processed_data['target_names'])}\")\n",
    "    \n",
    "    if 'version' in locals():\n",
    "        print(f\"   â€¢ Data version: {version}\")\n",
    "        print(f\"   â€¢ Storage location: gs://{BUCKET_NAME}/{PROCESSED_DATA_ROOT}/v{version}/\")\n",
    "    \n",
    "    print(\"\\nğŸš€ **Ready for Next Phase:**\")\n",
    "    next_steps = [\n",
    "        \"Phase 3: Model Training Pipeline\",\n",
    "        \"   â€¢ Create training scripts for classification models\",\n",
    "        \"   â€¢ Set up Vertex AI Custom Training jobs\",\n",
    "        \"   â€¢ Implement model evaluation and metrics\",\n",
    "        \"   â€¢ Register models in Vertex AI Model Registry\",\n",
    "        \"Phase 4: Pipeline Orchestration\",\n",
    "        \"   â€¢ Build end-to-end Kubeflow Pipelines\",\n",
    "        \"   â€¢ Automate data processing â†’ training â†’ evaluation\",\n",
    "        \"   â€¢ Set up model deployment pipeline\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ **MLOps Best Practices Implemented:**\")\n",
    "    practices = [\n",
    "        \"âœ… Data validation and quality assurance\",\n",
    "        \"âœ… Reproducible preprocessing pipeline\",\n",
    "        \"âœ… Data versioning and lineage tracking\",\n",
    "        \"âœ… Separation of train/validation/test data\",\n",
    "        \"âœ… Artifact storage and metadata management\",\n",
    "        \"âœ… Cloud-native data processing\",\n",
    "        \"âœ… Scalable and maintainable code structure\"\n",
    "    ]\n",
    "    \n",
    "    for practice in practices:\n",
    "        print(f\"   {practice}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ **Task 2.2: Data Processing Pipeline - COMPLETED!**\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Display summary\n",
    "display_pipeline_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
