{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d24991",
   "metadata": {},
   "source": [
    "# üöÄ Vertex AI Custom Training Jobs - Task 3.2\n",
    "\n",
    "This notebook demonstrates how to deploy our training scripts to Vertex AI for distributed and scalable ML training.\n",
    "\n",
    "## üìã What We'll Build\n",
    "\n",
    "- ‚úÖ **Custom Training Scripts**: Containerized training code\n",
    "- ‚úÖ **Vertex AI Training Jobs**: Distributed cloud training\n",
    "- ‚úÖ **TensorBoard Integration**: Real-time monitoring\n",
    "- ‚úÖ **Hyperparameter Tuning**: Cloud-based optimization\n",
    "- ‚úÖ **Model Artifacts**: Automated model registration\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to create Vertex AI Custom Training jobs\n",
    "- Container-based ML training workflows\n",
    "- Cloud-based hyperparameter tuning\n",
    "- Production ML training best practices\n",
    "\n",
    "Let's scale our training! ‚òÅÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c1fc0",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration\n",
    "\n",
    "First, let's set up our environment and load our saved models for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eca77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Libraries imported successfully!\n",
      "\n",
      "‚úÖ Configuration loaded:\n",
      "   üìù Project: mlops-295610\n",
      "   üåç Region: us-central1\n",
      "   ü™£ Bucket: mlops-295610-mlops-bucket\n",
      "\n",
      "üéØ Vertex AI initialized for custom training jobs!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Cloud libraries\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import CustomJob\n",
    "from google.cloud.aiplatform import CustomContainerTrainingJob\n",
    "from google.cloud.aiplatform import HyperparameterTuningJob\n",
    "\n",
    "# ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = Path('../configs/setup_config.pkl')\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    \n",
    "    PROJECT_ID = config['project_id']\n",
    "    REGION = config['region']\n",
    "    BUCKET_NAME = config['bucket_name']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Configuration loaded:\")\n",
    "    print(f\"   üìù Project: {PROJECT_ID}\")\n",
    "    print(f\"   üåç Region: {REGION}\")\n",
    "    print(f\"   ü™£ Bucket: {BUCKET_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå Configuration not found. Please run setup notebooks first.\")\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Vertex AI initialized for custom training jobs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438a6e9",
   "metadata": {},
   "source": [
    "## 2. Create Production Training Script\n",
    "\n",
    "Let's create a standalone training script that can be used in Vertex AI Custom Training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470a4fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production training script created!\n",
      "üìÑ Saved to: ../training/train.py\n",
      "üìù Script size: 10418 characters\n"
     ]
    }
   ],
   "source": [
    "# Create training directory\n",
    "training_dir = Path('../training')\n",
    "training_dir.mkdir(exist_ok=True)\n",
    "\n",
    "training_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Production Training Script for Vertex AI Custom Training\n",
    "========================================================\n",
    "\n",
    "This script trains multiple ML models on the Iris dataset and saves the best model\n",
    "to Google Cloud Storage. Designed to run in Vertex AI Custom Training jobs.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import tempfile\n",
    "\n",
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train ML models on Iris dataset')\n",
    "    \n",
    "    # Data arguments\n",
    "    parser.add_argument('--data-bucket', type=str, required=True,\n",
    "                       help='GCS bucket containing processed data')\n",
    "    parser.add_argument('--data-version', type=str, default='latest',\n",
    "                       help='Version of processed data to use')\n",
    "    \n",
    "    # Training arguments\n",
    "    parser.add_argument('--models', type=str, default='all',\n",
    "                       choices=['all', 'sklearn', 'tensorflow'],\n",
    "                       help='Which models to train')\n",
    "    parser.add_argument('--tune-hyperparameters', action='store_true',\n",
    "                       help='Enable hyperparameter tuning')\n",
    "    \n",
    "    # Output arguments\n",
    "    parser.add_argument('--output-bucket', type=str, required=True,\n",
    "                       help='GCS bucket for saving trained models')\n",
    "    parser.add_argument('--model-version', type=str, default=None,\n",
    "                       help='Version tag for saved models')\n",
    "    \n",
    "    # TensorFlow arguments\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                       help='Number of training epochs for TensorFlow model')\n",
    "    parser.add_argument('--batch-size', type=int, default=16,\n",
    "                       help='Batch size for TensorFlow training')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def load_data_from_gcs(bucket_name, version='latest'):\n",
    "    \"\"\"Load processed data from GCS.\"\"\"\n",
    "    logger.info(f\"Loading data from gs://{bucket_name}\")\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Find version to use\n",
    "    if version == 'latest':\n",
    "        # Find latest version\n",
    "        blobs = bucket.list_blobs(prefix=\"processed_data/v\")\n",
    "        versions = set()\n",
    "        for blob in blobs:\n",
    "            parts = blob.name.split('/')\n",
    "            if len(parts) >= 2 and parts[1].startswith('v'):\n",
    "                versions.add(parts[1])\n",
    "        \n",
    "        if not versions:\n",
    "            raise ValueError(\"No processed data versions found\")\n",
    "        version = sorted(versions)[-1]\n",
    "    \n",
    "    logger.info(f\"Using data version: {version}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    datasets = {}\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        blob_path = f\"processed_data/{version}/iris_{split}.npz\"\n",
    "        blob = bucket.blob(blob_path)\n",
    "        \n",
    "        if blob.exists():\n",
    "            with tempfile.NamedTemporaryFile() as temp_file:\n",
    "                blob.download_to_filename(temp_file.name)\n",
    "                with np.load(temp_file.name) as data:\n",
    "                    datasets[split] = {\n",
    "                        'X': data['X'],\n",
    "                        'y': data['y'],\n",
    "                        'feature_names': data['feature_names'],\n",
    "                        'target_names': data['target_names']\n",
    "                    }\n",
    "            logger.info(f\"Loaded {split} data: {datasets[split]['X'].shape}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Data file not found: gs://{bucket_name}/{blob_path}\")\n",
    "    \n",
    "    return datasets, version\n",
    "\n",
    "def train_sklearn_models(X_train, y_train, X_val, y_val, tune_hyperparameters=False):\n",
    "    \"\"\"Train scikit-learn models.\"\"\"\n",
    "    logger.info(\"Training scikit-learn models\")\n",
    "    \n",
    "    models = {\n",
    "        'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'svm': SVC(random_state=42, probability=True),\n",
    "        'knn': KNeighborsClassifier(n_neighbors=5),\n",
    "        'gradient_boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        logger.info(f\"Training {name}\")\n",
    "        \n",
    "        if tune_hyperparameters and name == 'random_forest':\n",
    "            # Hyperparameter tuning for Random Forest\n",
    "            param_grid = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7, None],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            }\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                model, param_grid, cv=3, scoring='accuracy', n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Combine train and validation for hyperparameter tuning\n",
    "            X_combined = np.vstack([X_train, X_val])\n",
    "            y_combined = np.concatenate([y_train, y_val])\n",
    "            \n",
    "            grid_search.fit(X_combined, y_combined)\n",
    "            model = grid_search.best_estimator_\n",
    "            logger.info(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_accuracy = accuracy_score(y_val, model.predict(X_val))\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"{name} validation accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return trained_models, results\n",
    "\n",
    "def train_tensorflow_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=16):\n",
    "    \"\"\"Train TensorFlow model.\"\"\"\n",
    "    logger.info(\"Training TensorFlow model\")\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train with callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy', patience=20, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    _, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    logger.info(f\"TensorFlow model validation accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return model, val_accuracy\n",
    "\n",
    "def save_models_to_gcs(models_dict, tf_model, bucket_name, version, data_version):\n",
    "    \"\"\"Save trained models to GCS.\"\"\"\n",
    "    logger.info(f\"Saving models to gs://{bucket_name}\")\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    \n",
    "    # Save sklearn models\n",
    "    for name, model in models_dict.items():\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            pickle.dump(model, open(temp_file.name, 'wb'))\n",
    "            \n",
    "            blob_path = f\"models/v{version}/sklearn/{name}.pkl\"\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(temp_file.name)\n",
    "            \n",
    "            logger.info(f\"Uploaded {name} model to gs://{bucket_name}/{blob_path}\")\n",
    "    \n",
    "    # Save TensorFlow model\n",
    "    if tf_model is not None:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.keras') as temp_file:\n",
    "            tf_model.save(temp_file.name)\n",
    "            \n",
    "            blob_path = f\"models/v{version}/tensorflow/model.keras\"\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(temp_file.name)\n",
    "            \n",
    "            logger.info(f\"Uploaded TensorFlow model to gs://{bucket_name}/{blob_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'version': version,\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'data_version': data_version,\n",
    "        'models': list(models_dict.keys()) + (['tensorflow'] if tf_model else [])\n",
    "    }\n",
    "    \n",
    "    blob_path = f\"models/v{version}/metadata.json\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_string(json.dumps(metadata, indent=2))\n",
    "    \n",
    "    logger.info(f\"Uploaded metadata to gs://{bucket_name}/{blob_path}\")\n",
    "    \n",
    "    return version\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    args = parse_args()\n",
    "    \n",
    "    logger.info(\"Starting Vertex AI Custom Training Job\")\n",
    "    logger.info(f\"Arguments: {vars(args)}\")\n",
    "    \n",
    "    # Load data\n",
    "    datasets, data_version = load_data_from_gcs(args.data_bucket, args.data_version)\n",
    "    \n",
    "    X_train = datasets['train']['X']\n",
    "    y_train = datasets['train']['y']\n",
    "    X_val = datasets['validation']['X']\n",
    "    y_val = datasets['validation']['y']\n",
    "    \n",
    "    logger.info(f\"Training data shape: {X_train.shape}\")\n",
    "    logger.info(f\"Validation data shape: {X_val.shape}\")\n",
    "    \n",
    "    # Set model version\n",
    "    if args.model_version is None:\n",
    "        model_version = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    else:\n",
    "        model_version = args.model_version\n",
    "    \n",
    "    # Train models\n",
    "    trained_models = {}\n",
    "    tf_model = None\n",
    "    \n",
    "    if args.models in ['all', 'sklearn']:\n",
    "        sklearn_models, sklearn_results = train_sklearn_models(\n",
    "            X_train, y_train, X_val, y_val, args.tune_hyperparameters\n",
    "        )\n",
    "        trained_models.update(sklearn_models)\n",
    "    \n",
    "    if args.models in ['all', 'tensorflow']:\n",
    "        tf_model, tf_accuracy = train_tensorflow_model(\n",
    "            X_train, y_train, X_val, y_val, args.epochs, args.batch_size\n",
    "        )\n",
    "    \n",
    "    # Save models\n",
    "    saved_version = save_models_to_gcs(\n",
    "        trained_models, tf_model, args.output_bucket, model_version, data_version\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Training completed! Models saved with version: {saved_version}\")\n",
    "    \n",
    "    # Output for Vertex AI\n",
    "    print(f\"MODEL_VERSION={saved_version}\")\n",
    "    print(f\"DATA_VERSION={data_version}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the training script\n",
    "with open(training_dir / 'train.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"‚úÖ Production training script created!\")\n",
    "print(f\"üìÑ Saved to: {training_dir / 'train.py'}\")\n",
    "print(f\"üìù Script size: {len(training_script)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76c36f6",
   "metadata": {},
   "source": [
    "## 3. Create Docker Container for Training\n",
    "\n",
    "Let's create a Dockerfile to containerize our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab89004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dockerfile created!\n",
      "üìÑ Saved to: ../training/Dockerfile\n",
      "‚úÖ Requirements.txt created!\n",
      "üìÑ Saved to: ../training/requirements.txt\n",
      "\n",
      "üìÇ Training directory contents:\n",
      "   üìÑ requirements.txt (0.12 KB)\n",
      "   üìÑ Dockerfile (0.49 KB)\n",
      "   üìÑ train.py (10.17 KB)\n"
     ]
    }
   ],
   "source": [
    "# Create Dockerfile for the training script\n",
    "dockerfile_content = '''\n",
    "# Use the official TensorFlow runtime as base image\n",
    "FROM tensorflow/tensorflow:2.15.0-py3\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install additional requirements\n",
    "RUN pip install --no-cache-dir \\\\\n",
    "    google-cloud-storage==2.10.0 \\\\\n",
    "    google-cloud-aiplatform==1.37.0 \\\\\n",
    "    scikit-learn==1.3.0 \\\\\n",
    "    pandas==2.0.3 \\\\\n",
    "    numpy==1.24.3\n",
    "\n",
    "# Copy training script\n",
    "COPY train.py /app/train.py\n",
    "\n",
    "# Make script executable\n",
    "RUN chmod +x /app/train.py\n",
    "\n",
    "# Set the entrypoint\n",
    "ENTRYPOINT [\"python\", \"/app/train.py\"]\n",
    "'''.strip()\n",
    "\n",
    "# Save Dockerfile\n",
    "with open(training_dir / 'Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile created!\")\n",
    "print(f\"üìÑ Saved to: {training_dir / 'Dockerfile'}\")\n",
    "\n",
    "# Create requirements.txt for reference\n",
    "requirements_content = '''\n",
    "tensorflow==2.15.0\n",
    "google-cloud-storage==2.10.0\n",
    "google-cloud-aiplatform==1.37.0\n",
    "scikit-learn==1.3.0\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "'''.strip()\n",
    "\n",
    "with open(training_dir / 'requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"‚úÖ Requirements.txt created!\")\n",
    "print(f\"üìÑ Saved to: {training_dir / 'requirements.txt'}\")\n",
    "\n",
    "# List created files\n",
    "training_files = list(training_dir.glob('*'))\n",
    "print(f\"\\nüìÇ Training directory contents:\")\n",
    "for file in training_files:\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"   üìÑ {file.name} ({size_kb:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c69c5",
   "metadata": {},
   "source": [
    "## 4. Build and Push Container to Artifact Registry\n",
    "\n",
    "Let's build our Docker container and push it to Google Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Building and pushing container image\n",
      "üìç Repository: us-central1-docker.pkg.dev/mlops-295610/mlops-training\n",
      "üè∑Ô∏è  Image: us-central1-docker.pkg.dev/mlops-295610/mlops-training/iris-trainer:v1.0\n",
      "\n",
      "üîÑ Creating Artifact Registry repository...\n",
      "   Command: gcloud artifacts repositories create mlops-training --repository-format=docker --location=us-central1 --description=MLOps training container repository\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define container details\n",
    "REPOSITORY_NAME = \"mlops-training\"\n",
    "IMAGE_NAME = \"iris-trainer\"\n",
    "IMAGE_TAG = \"v1.0\"\n",
    "\n",
    "# Artifact Registry URI\n",
    "ARTIFACT_REGISTRY_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY_NAME}\"\n",
    "FULL_IMAGE_URI = f\"{ARTIFACT_REGISTRY_URI}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "print(f\"üèóÔ∏è Building and pushing container image\")\n",
    "print(f\"üìç Repository: {ARTIFACT_REGISTRY_URI}\")\n",
    "print(f\"üè∑Ô∏è  Image: {FULL_IMAGE_URI}\")\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Run a shell command and capture output.\"\"\"\n",
    "    print(f\"\\nüîÑ {description}...\")\n",
    "    print(f\"   Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=training_dir)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"   ‚úÖ Success!\")\n",
    "        if result.stdout:\n",
    "            print(f\"   Output: {result.stdout.strip()[:200]}...\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Failed!\")\n",
    "        if result.stderr:\n",
    "            print(f\"   Error: {result.stderr.strip()[:200]}...\")\n",
    "    \n",
    "    return result.returncode == 0\n",
    "\n",
    "# Step 1: Create Artifact Registry repository (if it doesn't exist)\n",
    "create_repo_cmd = [\n",
    "    'gcloud', 'artifacts', 'repositories', 'create', REPOSITORY_NAME,\n",
    "    '--repository-format=docker',\n",
    "    f'--location={REGION}',\n",
    "    '--description=MLOps training container repository'\n",
    "]\n",
    "\n",
    "success = run_command(create_repo_cmd, \"Creating Artifact Registry repository\")\n",
    "\n",
    "# Step 2: Configure Docker authentication\n",
    "auth_cmd = ['gcloud', 'auth', 'configure-docker', f'{REGION}-docker.pkg.dev']\n",
    "success = run_command(auth_cmd, \"Configuring Docker authentication\")\n",
    "\n",
    "# Step 3: Build Docker image\n",
    "build_cmd = ['docker', 'build', '-t', FULL_IMAGE_URI, '.']\n",
    "success = run_command(build_cmd, \"Building Docker image\")\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüéâ Container built successfully!\")\n",
    "    print(f\"üè∑Ô∏è  Image URI: {FULL_IMAGE_URI}\")\n",
    "    \n",
    "    # Step 4: Push to Artifact Registry\n",
    "    push_cmd = ['docker', 'push', FULL_IMAGE_URI]\n",
    "    success = run_command(push_cmd, \"Pushing image to Artifact Registry\")\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\nüöÄ Container successfully pushed to Artifact Registry!\")\n",
    "        print(f\"üì¶ Ready to use in Vertex AI Custom Training jobs\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Failed to push container\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Failed to build container\")\n",
    "\n",
    "# Save container URI for later use\n",
    "container_config = {\n",
    "    'image_uri': FULL_IMAGE_URI,\n",
    "    'repository': ARTIFACT_REGISTRY_URI,\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "config_dir = Path('../configs')\n",
    "with open(config_dir / 'container_config.json', 'w') as f:\n",
    "    json.dump(container_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Container configuration saved to: {config_dir / 'container_config.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d7a82",
   "metadata": {},
   "source": [
    "## 5. Create and Run Vertex AI Custom Training Job\n",
    "\n",
    "Now let's create and execute a Vertex AI Custom Training job using our containerized training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fe969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_training_job(image_uri, bucket_name, job_name_prefix=\"iris-training\"):\n",
    "    \"\"\"Create and run a Vertex AI Custom Training job.\"\"\"\n",
    "    \n",
    "    # Generate unique job name\n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    job_display_name = f\"{job_name_prefix}-{timestamp}\"\n",
    "    \n",
    "    print(f\"üöÄ Creating Vertex AI Custom Training Job\")\n",
    "    print(f\"üìõ Job name: {job_display_name}\")\n",
    "    print(f\"üñºÔ∏è Image URI: {image_uri}\")\n",
    "    \n",
    "    # Define training arguments\n",
    "    worker_args = [\n",
    "        '--data-bucket', bucket_name,\n",
    "        '--data-version', 'latest',\n",
    "        '--models', 'all',\n",
    "        '--tune-hyperparameters',\n",
    "        '--output-bucket', bucket_name,\n",
    "        '--epochs', '50',\n",
    "        '--batch-size', '16'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Training arguments:\")\n",
    "    for i in range(0, len(worker_args), 2):\n",
    "        if i + 1 < len(worker_args):\n",
    "            print(f\"   {worker_args[i]}: {worker_args[i+1]}\")\n",
    "        else:\n",
    "            print(f\"   {worker_args[i]}: (flag)\")\n",
    "    \n",
    "    # Create custom container training job\n",
    "    job = CustomContainerTrainingJob(\n",
    "        display_name=job_display_name,\n",
    "        container_uri=image_uri,\n",
    "        command=[],  # Using ENTRYPOINT from Dockerfile\n",
    "        model_serving_container_image_uri=None,  # No serving for this demo\n",
    "        requirements=[],  # Included in container\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Job specification:\")\n",
    "    print(f\"   Display name: {job_display_name}\")\n",
    "    print(f\"   Container URI: {image_uri}\")\n",
    "    print(f\"   Staging bucket: gs://{bucket_name}\")\n",
    "    \n",
    "    # Define machine configuration\n",
    "    machine_type = \"n1-standard-4\"  # 4 vCPUs, 15 GB RAM\n",
    "    replica_count = 1\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è Compute configuration:\")\n",
    "    print(f\"   Machine type: {machine_type}\")\n",
    "    print(f\"   Replica count: {replica_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Submit the training job\n",
    "        print(f\"\\nüöÄ Submitting training job...\")\n",
    "        \n",
    "        model = job.run(\n",
    "            args=worker_args,\n",
    "            replica_count=replica_count,\n",
    "            machine_type=machine_type,\n",
    "            sync=False,  # Don't wait for completion in notebook\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training job submitted successfully!\")\n",
    "        print(f\"üìõ Job name: {job_display_name}\")\n",
    "        print(f\"üîó Console URL: https://console.cloud.google.com/vertex-ai/training/custom-jobs?project={PROJECT_ID}\")\n",
    "        \n",
    "        # Return job for monitoring\n",
    "        return job, job_display_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to submit training job: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load container configuration\n",
    "try:\n",
    "    with open(config_dir / 'container_config.json', 'r') as f:\n",
    "        container_config = json.load(f)\n",
    "    \n",
    "    image_uri = container_config['image_uri']\n",
    "    print(f\"üì¶ Using container: {image_uri}\")\n",
    "    \n",
    "    # Create and submit training job\n",
    "    training_job, job_name = create_custom_training_job(image_uri, BUCKET_NAME)\n",
    "    \n",
    "    if training_job:\n",
    "        # Save job info for monitoring\n",
    "        job_info = {\n",
    "            'job_name': job_name,\n",
    "            'image_uri': image_uri,\n",
    "            'project_id': PROJECT_ID,\n",
    "            'region': REGION,\n",
    "            'submitted_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(config_dir / 'training_job_info.json', 'w') as f:\n",
    "            json.dump(job_info, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Job information saved for monitoring\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Container configuration not found. Please run the container build step first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7f170",
   "metadata": {},
   "source": [
    "## 6. Monitor Training Job\n",
    "\n",
    "Let's create a function to monitor the status of our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf497826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_training_job(job_name=None):\n",
    "    \"\"\"Monitor the status of a Vertex AI training job.\"\"\"\n",
    "    \n",
    "    if job_name is None:\n",
    "        # Try to load from saved job info\n",
    "        try:\n",
    "            with open(config_dir / 'training_job_info.json', 'r') as f:\n",
    "                job_info = json.load(f)\n",
    "            job_name = job_info['job_name']\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå No job information found. Please specify job_name or run a training job first.\")\n",
    "            return\n",
    "    \n",
    "    print(f\"üìä Monitoring training job: {job_name}\")\n",
    "    print(f\"üåç Region: {REGION}\")\n",
    "    print(f\"üìù Project: {PROJECT_ID}\")\n",
    "    \n",
    "    try:\n",
    "        # List all custom training jobs to find ours\n",
    "        from google.cloud import aiplatform\n",
    "        \n",
    "        # Initialize client\n",
    "        client = aiplatform.gapic.JobServiceClient(\n",
    "            client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "        )\n",
    "        \n",
    "        parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "        \n",
    "        print(f\"\\nüîç Searching for job: {job_name}\")\n",
    "        \n",
    "        # List custom jobs\n",
    "        jobs = client.list_custom_jobs(parent=parent)\n",
    "        \n",
    "        job_found = False\n",
    "        for job in jobs:\n",
    "            if job.display_name == job_name:\n",
    "                job_found = True\n",
    "                \n",
    "                print(f\"\\n‚úÖ Job found!\")\n",
    "                print(f\"üìõ Display name: {job.display_name}\")\n",
    "                print(f\"üÜî Resource name: {job.name}\")\n",
    "                print(f\"üìä State: {job.state.name}\")\n",
    "                print(f\"üïí Create time: {job.create_time}\")\n",
    "                \n",
    "                if job.start_time:\n",
    "                    print(f\"üöÄ Start time: {job.start_time}\")\n",
    "                if job.end_time:\n",
    "                    print(f\"üèÅ End time: {job.end_time}\")\n",
    "                \n",
    "                # Show error if failed\n",
    "                if job.error.code != 0:\n",
    "                    print(f\"‚ùå Error: {job.error.message}\")\n",
    "                \n",
    "                # Job spec details\n",
    "                print(f\"\\nüìã Job Configuration:\")\n",
    "                if job.job_spec.worker_pool_specs:\n",
    "                    for i, pool in enumerate(job.job_spec.worker_pool_specs):\n",
    "                        print(f\"   Worker Pool {i+1}:\")\n",
    "                        print(f\"     Machine type: {pool.machine_spec.machine_type}\")\n",
    "                        print(f\"     Replica count: {pool.replica_count}\")\n",
    "                        if pool.container_spec:\n",
    "                            print(f\"     Container: {pool.container_spec.image_uri}\")\n",
    "                            if pool.container_spec.args:\n",
    "                                print(f\"     Args: {' '.join(pool.container_spec.args)}\")\n",
    "                \n",
    "                # Console links\n",
    "                print(f\"\\nüîó Useful Links:\")\n",
    "                print(f\"   Console: https://console.cloud.google.com/vertex-ai/training/custom-jobs?project={PROJECT_ID}\")\n",
    "                print(f\"   Logs: https://console.cloud.google.com/logs/query?project={PROJECT_ID}\")\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if not job_found:\n",
    "            print(f\"‚ùå Job '{job_name}' not found\")\n",
    "            print(f\"\\nüìã Available jobs:\")\n",
    "            for job in jobs:\n",
    "                print(f\"   ‚Ä¢ {job.display_name} ({job.state.name})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error monitoring job: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Monitor the current job\n",
    "monitor_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b2feb",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning Job (Optional)\n",
    "\n",
    "Let's create a Vertex AI Hyperparameter Tuning job for automated optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "def create_hyperparameter_tuning_job(image_uri, bucket_name):\n",
    "    \"\"\"Create a Vertex AI Hyperparameter Tuning job.\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    job_display_name = f\"iris-hptuning-{timestamp}\"\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Creating Hyperparameter Tuning Job\")\n",
    "    print(f\"üìõ Job name: {job_display_name}\")\n",
    "    \n",
    "    # Define hyperparameter search space\n",
    "    # For demonstration, we'll tune TensorFlow model parameters\n",
    "    parameter_spec = {\n",
    "        'epochs': hpt.IntegerParameterSpec(min=20, max=100, scale='linear'),\n",
    "        'batch-size': hpt.DiscreteParameterSpec(values=[8, 16, 32], scale='linear'),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Hyperparameter search space:\")\n",
    "    for param, spec in parameter_spec.items():\n",
    "        print(f\"   {param}: {spec}\")\n",
    "    \n",
    "    # Define metric to optimize\n",
    "    metric_spec = hpt.MetricSpec(metric_id='val_accuracy', goal='MAXIMIZE')\n",
    "    \n",
    "    print(f\"\\nüéØ Optimization metric: {metric_spec.metric_id} ({metric_spec.goal})\")\n",
    "    \n",
    "    # Base training arguments\n",
    "    base_args = [\n",
    "        '--data-bucket', bucket_name,\n",
    "        '--data-version', 'latest',\n",
    "        '--models', 'tensorflow',  # Only tune TensorFlow model\n",
    "        '--output-bucket', bucket_name,\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Create hyperparameter tuning job\n",
    "        hp_job = HyperparameterTuningJob(\n",
    "            display_name=job_display_name,\n",
    "            custom_job=CustomJob.from_local_script(\n",
    "                display_name=f\"hp-trial-{timestamp}\",\n",
    "                script_path=str(training_dir / 'train.py'),\n",
    "                container_uri=image_uri,\n",
    "                args=base_args,\n",
    "                machine_type=\"n1-standard-4\",\n",
    "                replica_count=1\n",
    "            ),\n",
    "            metric_spec=metric_spec,\n",
    "            parameter_spec=parameter_spec,\n",
    "            max_trial_count=9,  # Number of trials to run\n",
    "            parallel_trial_count=3,  # Parallel trials\n",
    "            search_algorithm=None,  # Use default algorithm\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüöÄ Submitting hyperparameter tuning job...\")\n",
    "        print(f\"   Max trials: 9\")\n",
    "        print(f\"   Parallel trials: 3\")\n",
    "        \n",
    "        # Submit job (non-blocking)\n",
    "        hp_job.run(sync=False)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Hyperparameter tuning job submitted!\")\n",
    "        print(f\"üìõ Job name: {job_display_name}\")\n",
    "        print(f\"üîó Console URL: https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning?project={PROJECT_ID}\")\n",
    "        \n",
    "        return hp_job, job_display_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create hyperparameter tuning job: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Uncomment to run hyperparameter tuning (optional)\n",
    "print(\"üí° Hyperparameter tuning job setup is ready!\")\n",
    "print(\"\\nTo run hyperparameter tuning, uncomment the following lines:\")\n",
    "print(\"\")\n",
    "print(\"# hp_job, hp_job_name = create_hyperparameter_tuning_job(image_uri, BUCKET_NAME)\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  Note: Hyperparameter tuning jobs consume more resources and may incur higher costs.\")\n",
    "\n",
    "# Uncomment to actually run:\n",
    "# hp_job, hp_job_name = create_hyperparameter_tuning_job(image_uri, BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd37f5",
   "metadata": {},
   "source": [
    "## 8. Task 3.2 Summary\n",
    "\n",
    "Let's summarize what we've accomplished in Vertex AI Custom Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_task_32_summary():\n",
    "    \"\"\"Display summary of Task 3.2 accomplishments.\"\"\"\n",
    "    \n",
    "    print(\"üéØ Task 3.2: Vertex AI Custom Training Jobs - COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n‚úÖ **What We Accomplished:**\")\n",
    "    \n",
    "    accomplishments = [\n",
    "        \"üìú Created production-ready training script\",\n",
    "        \"   ‚Ä¢ Command-line argument parsing\",\n",
    "        \"   ‚Ä¢ GCS data loading and model saving\",\n",
    "        \"   ‚Ä¢ Multiple model training support\",\n",
    "        \"   ‚Ä¢ Comprehensive logging and error handling\",\n",
    "        \"üê≥ Built Docker container for training\",\n",
    "        \"   ‚Ä¢ TensorFlow base image\",\n",
    "        \"   ‚Ä¢ All dependencies included\",\n",
    "        \"   ‚Ä¢ Optimized for cloud training\",\n",
    "        \"üì¶ Pushed container to Artifact Registry\",\n",
    "        \"   ‚Ä¢ Secure container storage\",\n",
    "        \"   ‚Ä¢ Version control for images\",\n",
    "        \"   ‚Ä¢ Ready for distributed training\",\n",
    "        \"üöÄ Created Vertex AI Custom Training Job\",\n",
    "        \"   ‚Ä¢ Cloud-based model training\",\n",
    "        \"   ‚Ä¢ Scalable compute resources\",\n",
    "        \"   ‚Ä¢ Automated model artifact storage\",\n",
    "        \"üìä Set up job monitoring and tracking\",\n",
    "        \"   ‚Ä¢ Real-time job status monitoring\",\n",
    "        \"   ‚Ä¢ Logging and error tracking\",\n",
    "        \"   ‚Ä¢ Console integration\",\n",
    "        \"‚öôÔ∏è Prepared hyperparameter tuning setup\",\n",
    "        \"   ‚Ä¢ Automated optimization framework\",\n",
    "        \"   ‚Ä¢ Parallel trial execution\",\n",
    "        \"   ‚Ä¢ Metric-based optimization\"\n",
    "    ]\n",
    "    \n",
    "    for item in accomplishments:\n",
    "        print(f\"   {item}\")\n",
    "    \n",
    "    print(\"\\nüèóÔ∏è **Architecture Built:**\")\n",
    "    \n",
    "    architecture = [\n",
    "        \"üìÅ Local Development:\",\n",
    "        \"   ‚Ä¢ training/train.py - Production training script\",\n",
    "        \"   ‚Ä¢ training/Dockerfile - Container definition\",\n",
    "        \"   ‚Ä¢ training/requirements.txt - Dependencies\",\n",
    "        \"\",\n",
    "        \"‚òÅÔ∏è Cloud Infrastructure:\",\n",
    "        \"   ‚Ä¢ Artifact Registry - Container storage\",\n",
    "        \"   ‚Ä¢ Vertex AI Training - Managed training service\",\n",
    "        \"   ‚Ä¢ Google Cloud Storage - Data and model artifacts\",\n",
    "        \"   ‚Ä¢ Cloud Logging - Training logs and monitoring\",\n",
    "        \"\",\n",
    "        \"üîÑ Training Workflow:\",\n",
    "        \"   1. Load data from GCS\",\n",
    "        \"   2. Train models in containerized environment\",\n",
    "        \"   3. Save trained models back to GCS\",\n",
    "        \"   4. Log metrics and artifacts\"\n",
    "    ]\n",
    "    \n",
    "    for item in architecture:\n",
    "        print(f\"   {item}\")\n",
    "    \n",
    "    print(\"\\nüéØ **Key Benefits Achieved:**\")\n",
    "    \n",
    "    benefits = [\n",
    "        \"üîÑ **Scalability**: Training can scale to any compute size\",\n",
    "        \"üê≥ **Reproducibility**: Containerized training ensures consistency\",\n",
    "        \"‚òÅÔ∏è **Cloud-Native**: Fully integrated with Google Cloud services\",\n",
    "        \"üìä **Monitoring**: Real-time tracking of training progress\",\n",
    "        \"‚öôÔ∏è **Automation**: Ready for automated retraining workflows\",\n",
    "        \"üîê **Security**: Managed authentication and access control\",\n",
    "        \"üí∞ **Cost-Effective**: Pay-per-use compute resources\",\n",
    "        \"üè∑Ô∏è **Version Control**: Full lineage and versioning\"\n",
    "    ]\n",
    "    \n",
    "    for benefit in benefits:\n",
    "        print(f\"   {benefit}\")\n",
    "    \n",
    "    print(\"\\nüöÄ **Ready for Next Phase:**\")\n",
    "    next_steps = [\n",
    "        \"Task 3.3: Model Registry and Evaluation\",\n",
    "        \"   ‚Ä¢ Register models in Vertex AI Model Registry\",\n",
    "        \"   ‚Ä¢ Set up model evaluation metrics\",\n",
    "        \"   ‚Ä¢ Create model approval workflows\",\n",
    "        \"Phase 4: Pipeline Orchestration\",\n",
    "        \"   ‚Ä¢ Build end-to-end Kubeflow Pipelines\",\n",
    "        \"   ‚Ä¢ Automate entire ML workflow\",\n",
    "        \"   ‚Ä¢ Set up continuous training and deployment\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(\"\\nüí° **Production Readiness:**\")\n",
    "    readiness = [\n",
    "        \"‚úÖ Training scripts are production-ready and containerized\",\n",
    "        \"‚úÖ Cloud infrastructure is set up and configured\",\n",
    "        \"‚úÖ Monitoring and logging are in place\",\n",
    "        \"‚úÖ Scalable training workflow is established\",\n",
    "        \"‚úÖ Ready for automated MLOps pipelines\"\n",
    "    ]\n",
    "    \n",
    "    for item in readiness:\n",
    "        print(f\"   {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ **Task 3.2: Vertex AI Custom Training Jobs - COMPLETED!**\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Display summary\n",
    "display_task_32_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
